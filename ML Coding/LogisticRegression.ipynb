{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNJ5qcTgLoBk"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate = 0.01, num_iter = 1000):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.W = None\n",
        "    self.num_iter = num_iter\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    X = np.hstack([np.ones((len(X), 1)), X])\n",
        "    self.W = np.zeros(X.shape[1])\n",
        "\n",
        "    for i in range(self.num_iter):\n",
        "      z = np.dot(X, self.W)\n",
        "      y_pred = self._sigmoid(z)\n",
        "      cost = -1/len(X) * np.sum(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
        "      dw = 1/len(X) * np.dot((y_pred - y), X)\n",
        "\n",
        "      #Update\n",
        "      self.W = self.W - self.learning_rate*dw\n",
        "\n",
        "  def predict(self, X):\n",
        "    X = np.hstack([np.ones((len(X), 1)), X])\n",
        "    z = np.dot(X, self.W)\n",
        "    y_pred = self._sigmoid(z)\n",
        "    y_pred = np.round(y_pred).astype(int)\n",
        "    return y_pred\n",
        "\n",
        "  def _sigmoid(self, z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gKO4x9iaMGul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
        "y = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "X_new = np.array([[6, 7], [7, 8]])\n",
        "y_pred = log_reg.predict(X_new)\n",
        "\n",
        "print(y_pred)  # [1, 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-v_vylvQ-1K",
        "outputId": "6da5db1b-96e5-4a1f-be41-200236da04b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression2:\n",
        "  def __init__(self, learning_rate = 0.01, n_iters = 1000, regul = '12', reg_stren = 0.1, batch_size = 32):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.n_iter = n_iters\n",
        "    self.regul = regul\n",
        "    self.reg_stren = reg_stren\n",
        "    self.bs = batch_size\n",
        "    self.W = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    X = np.hstack([np.ones((len(X),1)), X])\n",
        "    self.W = np.zeros((X.shape[1]))\n",
        "    n_batches = len(X) // self.bs\n",
        "\n",
        "    for i in range(self.n_iter):\n",
        "      batch_indices = np.random.choice(len(X), self.bs)\n",
        "      X_batch = X[batch_indices]\n",
        "      y_batch = y[batch_indices]\n",
        "\n",
        "      z = np.dot(X_batch, self.W)\n",
        "      y_pred = self._sigmoid(z)\n",
        "      cost = (-1/self.bs)*(np.sum(y_batch * np.log(y_pred) + (1 - y_batch) * np.log(1 - y_pred)))\n",
        "      dw = (1/self.bs) * np.dot(X_batch.T, (y_pred - y_batch))\n",
        "\n",
        "      if self.regul == '12':\n",
        "        reg_cost = (1/(2*self.bs))*self.reg_stren * np.sum(self.W**2)\n",
        "        dw += self.reg_stren/self.bs * self.W\n",
        "        cost += reg_cost\n",
        "      else:\n",
        "        reg_cost = (1/(2*self.bs))*self.reg_stren * np.sum(np.abs(self.W))\n",
        "        cost += reg_cost\n",
        "        dw += self.reg_stren/self.bs * np.sign(self.W)\n",
        "\n",
        "      self.W -= self.learning_rate * dw\n",
        "\n",
        "  def predict(self, X):\n",
        "    X = np.hstack([np.ones((len(X), 1)), X])\n",
        "    z = np.dot(X, self.W)\n",
        "    y_pred = self._sigmoid(z)\n",
        "    return np.round(y_pred).astype(int)\n",
        "\n",
        "  def _sigmoid(self, z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n"
      ],
      "metadata": {
        "id": "9gAzHswARcWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
        "y = np.array([0, 0, 1, 1, 1])\n",
        "\n",
        "log_reg2 = LogisticRegression2(learning_rate=0.01, n_iters=1000, regul='l2', reg_stren=0.1, batch_size=2)\n",
        "\n",
        "log_reg2.fit(X, y)\n",
        "\n",
        "X_new = np.array([[6, 7], [7, 8]])\n",
        "y_pred = log_reg2.predict(X_new)\n",
        "\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2jIvFiDhAa_",
        "outputId": "323d9d2d-1e49-4ef9-d933-44cf508d2d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zr8M0iFSkPS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}